{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printRecordsPerPartition: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// because we'll need it later\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Utility method to count & print the number of records in each partition.\n",
    "def printRecordsPerPartition(df:org.apache.spark.sql.Dataset[Row]):Unit = {\n",
    "  println(\"Per-Partition Counts:\")\n",
    "  val results = df.rdd                                   // Convert to an RDD\n",
    "    .mapPartitions(it => Array(it.size).iterator, true)  // For each partition, count\n",
    "    .collect()                                           // Return the counts to the driver\n",
    "\n",
    "  results.foreach(x => println(\"* \" + x))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.SparkSession@51c54b69\n"
     ]
    }
   ],
   "source": [
    "println(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkContext@691ea435"
     ]
    }
   ],
   "source": [
    "print(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile = file:///Users/navaro/zeppelin-0.7.3-bin-all/data/pageviews-by-second-tsv.bz2\n",
       "tempDF = [_c0: string, _c1: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, _c1: string ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A reference to our tab-seperated-file\n",
    "val csvFile = \"file:///Users/navaro/zeppelin-0.7.3-bin-all/data/pageviews-by-second-tsv.bz2\"\n",
    "\n",
    "var tempDF = spark.read       // The DataFrameReader\n",
    "  .option(\"delimiter\", \"\\t\")  // Use tab delimiter (default is comma-separator)\n",
    "  .csv(csvFile)               // Creates a DataFrame from CSV after reading in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- requests: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read                    // The DataFrameReader\n",
    "  .option(\"delimiter\", \"\\t\")  // Use tab delimiter (default is comma-separator)\n",
    "  .option(\"header\", \"true\")   // Use first line of all files as header\n",
    "  .csv(csvFile)               // Creates a DataFrame from CSV after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root                                                                            \n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read                        // The DataFrameReader\n",
    "  .option(\"header\", \"true\")       // Use first line of all files as header\n",
    "  .option(\"delimiter\", \"\\t\")      // Use tab delimiter (default is comma-separator)\n",
    "  .option(\"inferSchema\", \"true\")  // Automatically infer data types\n",
    "  .csv(csvFile)                   // Creates a DataFrame from CSV after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvSchema = StructType(StructField(timestamp,StringType,false), StructField(site,StringType,false), StructField(requests,IntegerType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(timestamp,StringType,false), StructField(site,StringType,false), StructField(requests,IntegerType,false))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Required for StructField, StringType, IntegerType, etc.\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val csvSchema = StructType(\n",
    "  List(\n",
    "    StructField(\"timestamp\", StringType, false),\n",
    "    StructField(\"site\", StringType, false),\n",
    "    StructField(\"requests\", IntegerType, false)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read                    // The DataFrameReader\n",
    "  .option(\"header\", \"true\")   // Ignore line #1 - it's a header\n",
    "  .option(\"delimiter\", \"\\t\")  // Use tab delimiter (default is comma-separator)\n",
    "  .schema(csvSchema)          // Use the specified schema\n",
    "  .csv(csvFile)               // Creates a DataFrame from CSV after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 8\n",
      "Per-Partition Counts:\n",
      "* 1077215                                                                       \n",
      "* 999868\n",
      "* 999737\n",
      "* 999235\n",
      "* 999441\n",
      "* 985819\n",
      "* 1002541\n",
      "* 136144\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvDF = [timestamp: string, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: string, site: string ... 1 more field]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvDF = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"delimiter\", \"\\t\")\n",
    "  .schema(csvSchema)\n",
    "  .csv(csvFile)\n",
    "\n",
    "printf(\"Partitions: %,d%n\", csvDF.rdd.partitions.size)\n",
    "printRecordsPerPartition(csvDF)\n",
    "println(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- channel: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- delta: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- geocoding: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- countryCode2: string (nullable = true)\n",
      " |    |-- countryCode3: string (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |    |-- stateProvince: string (nullable = true)\n",
      " |-- isAnonymous: boolean (nullable = true)\n",
      " |-- isNewPage: boolean (nullable = true)\n",
      " |-- isRobot: boolean (nullable = true)\n",
      " |-- isUnpatrolled: boolean (nullable = true)\n",
      " |-- namespace: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- pageURL: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- userURL: string (nullable = true)\n",
      " |-- wikipedia: string (nullable = true)\n",
      " |-- wikipediaURL: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonFile = file:///Users/navaro/zeppelin-0.7.3-bin-all/data/snapshot.json\n",
       "wikiEditsDF = [channel: string, comment: string ... 16 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[channel: string, comment: string ... 16 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonFile = \"file:///Users/navaro/zeppelin-0.7.3-bin-all/data/snapshot.json\"\n",
    "\n",
    "val wikiEditsDF = spark.read        // The DataFrameReader\n",
    "    .option(\"inferSchema\", \"true\")  // Automatically infer data types & column names\n",
    "    .json(jsonFile)                 // Creates a DataFrame from JSON after reading in the file\n",
    "\n",
    "wikiEditsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonSchema = StructType(StructField(channel,StringType,true), StructField(comment,StringType,true), StructField(delta,IntegerType,true), StructField(flag,StringType,true), StructField(geocoding,StructType(StructField(city,StringType,true), StructField(country,StringType,true), StructField(countryCode2,StringType,true), StructField(countryCode3,StringType,true), StructField(stateProvince,StringType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true)),true), StructField(isAnonymous,BooleanType,true), StructField(isNewPage,BooleanType,true), StructField(isRobot,BooleanType,true), StructField(isUnpatrolled,BooleanType,true), StructField(namespace,StringType,true), StructField(page,StringType,true), StructField(pageURL,S...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(channel,StringType,true), StructField(comment,StringType,true), StructField(delta,IntegerType,true), StructField(flag,StringType,true), StructField(geocoding,StructType(StructField(city,StringType,true), StructField(country,StringType,true), StructField(countryCode2,StringType,true), StructField(countryCode3,StringType,true), StructField(stateProvince,StringType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true)),true), StructField(isAnonymous,BooleanType,true), StructField(isNewPage,BooleanType,true), StructField(isRobot,BooleanType,true), StructField(isUnpatrolled,BooleanType,true), StructField(namespace,StringType,true), StructField(page,StringType,true), StructField(pageURL,StringType,true), StructField(timestamp,StringType,true), StructField(url,StringType,true), StructField(user,StringType,true), StructField(userURL,StringType,true), StructField(wikipediaURL,StringType,true), StructField(wikipedia,StringType,true))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonSchema = StructType(List(\n",
    "  StructField(\"channel\", StringType, true),\n",
    "  StructField(\"comment\", StringType, true),\n",
    "  StructField(\"delta\", IntegerType, true),\n",
    "  StructField(\"flag\", StringType, true),\n",
    "  StructField(\"geocoding\", StructType(List(\n",
    "    StructField(\"city\", StringType, true),\n",
    "    StructField(\"country\", StringType, true),\n",
    "    StructField(\"countryCode2\", StringType, true),\n",
    "    StructField(\"countryCode3\", StringType, true),\n",
    "    StructField(\"stateProvince\", StringType, true),\n",
    "    StructField(\"latitude\", DoubleType, true),\n",
    "    StructField(\"longitude\", DoubleType, true)\n",
    "  )), true),\n",
    "  StructField(\"isAnonymous\", BooleanType, true),\n",
    "  StructField(\"isNewPage\", BooleanType, true),\n",
    "  StructField(\"isRobot\", BooleanType, true),\n",
    "  StructField(\"isUnpatrolled\", BooleanType, true),\n",
    "  StructField(\"namespace\", StringType, true),\n",
    "  StructField(\"page\", StringType, true),\n",
    "  StructField(\"pageURL\", StringType, true),\n",
    "  StructField(\"timestamp\", StringType, true),\n",
    "  StructField(\"url\", StringType, true),\n",
    "  StructField(\"user\", StringType, true),\n",
    "  StructField(\"userURL\", StringType, true),\n",
    "  StructField(\"wikipediaURL\", StringType, true),\n",
    "  StructField(\"wikipedia\", StringType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- channel: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- delta: integer (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- geocoding: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- countryCode2: string (nullable = true)\n",
      " |    |-- countryCode3: string (nullable = true)\n",
      " |    |-- stateProvince: string (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- isAnonymous: boolean (nullable = true)\n",
      " |-- isNewPage: boolean (nullable = true)\n",
      " |-- isRobot: boolean (nullable = true)\n",
      " |-- isUnpatrolled: boolean (nullable = true)\n",
      " |-- namespace: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- pageURL: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- userURL: string (nullable = true)\n",
      " |-- wikipediaURL: string (nullable = true)\n",
      " |-- wikipedia: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read             // The DataFrameReader\n",
    "  .schema(jsonSchema)  // Use the specified schema\n",
    "  .json(jsonFile)      // Creates a DataFrame from JSON after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 1\n",
      "Per-Partition Counts:\n",
      "* 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonDF = [channel: string, comment: string ... 16 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[channel: string, comment: string ... 16 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonDF = spark.read\n",
    "  .schema(jsonSchema)\n",
    "  .json(jsonFile)\n",
    "\n",
    "printf(\"Partitions: %,d%n\", jsonDF.rdd.partitions.size)\n",
    "printRecordsPerPartition(jsonDF)\n",
    "println(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textFile = file:///Users/navaro/zeppelin-0.7.3-bin-all/data/tom.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "file:///Users/navaro/zeppelin-0.7.3-bin-all/data/tom.txt"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = \"file:///Users/navaro/zeppelin-0.7.3-bin-all/data/tom.txt\"\n",
    "\n",
    "spark.read         // The DataFrameReader\n",
    "  .text(textFile)  // Creates a DataFrame from raw text after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project: string (nullable = true)\n",
      " |-- article: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      " |-- bytes_served: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parquetFile = file:///Users/navaro/zeppelin-0.7.3-bin-all/data/pagecounts-parquet/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "file:///Users/navaro/zeppelin-0.7.3-bin-all/data/pagecounts-parquet/"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquetFile = \"file:///Users/navaro/zeppelin-0.7.3-bin-all/data/pagecounts-parquet/\"\n",
    "\n",
    "spark.read               // The DataFrameReader\n",
    "  .parquet(parquetFile)  // Creates a DataFrame from Parquet after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project: string (nullable = true)\n",
      " |-- article: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      " |-- bytes_served: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parquetSchema = StructType(StructField(project,StringType,false), StructField(article,StringType,false), StructField(requests,IntegerType,false), StructField(bytes_served,LongType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(project,StringType,false), StructField(article,StringType,false), StructField(requests,IntegerType,false), StructField(bytes_served,LongType,false))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquetSchema = StructType(\n",
    "  List(\n",
    "    StructField(\"project\", StringType, false),\n",
    "    StructField(\"article\", StringType, false),\n",
    "    StructField(\"requests\", IntegerType, false),\n",
    "    StructField(\"bytes_served\", LongType, false)\n",
    "  )\n",
    ")\n",
    "\n",
    "spark.read                // The DataFrameReader\n",
    "  .schema(parquetSchema)  // Use the specified schema\n",
    "  .parquet(parquetFile)   // Creates a DataFrame from Parquet after reading in the file\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 8\n",
      "Per-Partition Counts:\n",
      "* 1161100                                                                       \n",
      "* 1111411\n",
      "* 999869\n",
      "* 724384\n",
      "* 725313\n",
      "* 625841\n",
      "* 536227\n",
      "* 386797\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parquetDF = [project: string, article: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[project: string, article: string ... 2 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquetDF = spark.read.schema(parquetSchema).parquet(parquetFile)\n",
    "\n",
    "printf(\"Partitions: %,d%n\", parquetDF.rdd.partitions.size)\n",
    "printRecordsPerPartition(parquetDF)\n",
    "println(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
